
This is a journal of testing the prototype of the new fast LINC backend --
linc\_max.

----[06/02/2014 07:56]----------------------------------------------------------

The linc\_max module has two testing functions:

update(N) - load the flow table from the file named priv/testN.tab
start() - starts the testing switch; 

update() can be called while the switch is running.

# Simplest test

The simplest test replicates the behaviour of the nullx forwarder but using the
dynamic flow table generation. The flow specifications for the experiment:

```
{flow,
	[{in_port,1}],

	{instr,undefined,
		   undefined,
		   {write,[{output,2}]},
		   undefined,
		   undefined}}.

{flow,
	[{in_port,2}],
	
	{instr,undefined,
		   undefined,
		   {write,[{output,1}]},
		   undefined,
		   undefined}}.
```

The processing delay for the setup is encouraging

PL (all) = 3.52 +/- 0.38 us (95%)
PL (to) = 3.16 +/- 0.32 us (95%)
PL (from) = 3.88 +/- 0.61 us (95%)

It is even lower than the previously established baseline. The probable cause is
vcpu pinning. The output of 'xl vcpu-list':

Name                                ID  VCPU   CPU State   Time(s) CPU Affinity
Domain-0                             0     0    5   -b-    3380.4  0-6
Domain-0                             0     1    1   -b-    1797.6  0-6
Domain-0                             0     2    6   -b-     756.6  0-6
Domain-0                             0     3    3   -b-    4295.3  0-6
Domain-0                             0     4    6   -b-    4095.7  0-6
Domain-0                             0     5    4   -b-    1238.8  0-6
Domain-0                             0     6    2   -b-    3023.7  0-6
Domain-0                             0     7    0   r--    2829.9  0-6
vm2                                169     0    0   -b-    2261.2  0-6
vm1                                171     0    5   -b-    1301.2  0-6
lincx                              471     0    7   r--     183.0  7

lincx domain has the physical core #7 all for its own use.

3.5us looks like a new baseline processing delay.

The following flow table (test1.tab) shows forwards ICMP packets (pings) to the
controller. Thus linc\_max:update(1) stops pings between vm1 and vm2.

{flow,
	[{in_port,1},
	 {ip_proto,1}],

	{instr,undefined,
		   undefined,
		   {write,[{output,controller}]},
		   undefined,
		   undefined}}.

# More flows - 1

The test2.tab flow table contains 130 flows, 65 in each direction. The matching
is done on in_port and ipv4_dst fields. There are two flows to allow ARP traffic
throw. The latency is as follows:

PL (all) = 6.27 +/- 1.26 us (95%)
PL (to) = 5.30 +/- 1.70 us (95%)
PL (from) = 7.23 +/- 1.67 us (95%)

Not that the variation of results is high. Let us rerun the test.

PL (all) = 6.44 +/- 1.82 us (95%)
PL (to) = 3.11 +/- 0.63 us (95%)
PL (from) = 9.77 +/- 2.08 us (95%)

Results are similar and variation is even bigger.

# More flows - 2

The test3.tab was generated by scripts/genflow using the following spec:

```
{num_flows,128}					%% 128 flows in one direction, 256 total
{match,{1.0,eth_dst,16#ffffff}}	%% for 100% of flows match last 3 bytes of dst MAC
{match,{0.5,vlan_vid,nomask}}	%% for 50% of flows match vlan_vid
{match,{0.1,ip_dscp}}			%% for 10% of flows match DSCP field (v4 or v6)
allow_arp						%% add a flow to pass ARP traffic through
```

PL (all) = 5.00 +/- 0.68 us (95%)
PL (to) = 4.35 +/- 0.61 us (95%)
PL (from) = 5.64 +/- 1.07 us (95%)

The results are more stable. It looks like 256 flows add 1.5us to processing
delay.

----[08/02/2014 23:19]----------------------------------------------------------

The restaring scheme described in fast\_path.md added to linc\_max. The scheme
avoids the garbage collection which is not suitable for the fast path most of
the time.

The switch was configured with priv/test3.tab (same as above). The iperf
measuring TCP throughput was running for 24 straight hours. The summary line of
iperf is:

[  3]  0.0-86400.0 sec  0.00 � ��s  2.60 Gbits/sec

The strange markings is the failed attempt by iperf to display a number close
to 25Tb.

VM statistics after 24 hours of testing:

```
> memory().
[{total,74385384},
 {processes,31436800},
 {system,42948584},
 {processes_used,399796},
 {atom,1217700},
 {atom_used,1019660},
 {binary,38334464},
 {code,3363652},
 {ets,32768}]
```

The lincx domain had 1G of RAM. 74M of it is in use.

```
> i().
Pid                   Initial Call                          Heap     Reds Msgs
Registered            Current Function                     Stack
...
<0.1776628.0>         linc_max:'-plug/3-fun-0-'/0        1401290   119682    0
                      linc_max:plug/3                          5
...
```

The fast path process has been restarted 1,776,000 times.

More statistics for the (current) fast path process:

```
48> erlang:process_info(pid(0,1776628,0)).
[{current_function,{linc_max,plug,3}},
 {initial_call,{linc_max,'-plug/3-fun-0-',0}},
 {status,waiting},
 {message_queue_len,0},
 {messages,[]},
 {links,[#Port<0.3.0>,#Port<0.4.0>]},
 {dictionary,[]},
 {trap_exit,false},
 {error_handler,error_handler},
 {priority,normal},
 {group_leader,<0.67.0>},
 {total_heap_size,1459675},
 {heap_size,1459670},
 {stack_size,5},
 {reductions,124622},
 {garbage_collection,undefined}]
49> 
```

```
11> statistics(context_switches).
{1127314464,0}
```

```
12> statistics(reductions).
{465775674692,465775674692}
```

```
14> statistics(garbage_collection).
{266461,2483597668,0}
```

On average a GC run reclaimed about 9.3k words. This is a good number.

```
15> statistics(io).
{{input,358},{output,14772550}}
```

The vif ports do not update io statistics - filed as a bug.

```
16> statistics(runtime).
{68861550,-1391732804105}
```

The second number cannot be negative - filed as a bug.

```
18> statistics(wall_clock).
{93592800,93592713}
```

```
24> io:format("~p", [erlang:loaded()]).
[inet_parse,inet_db,dict,lists,filename,inet_config,embedded_export,file,
 hipe_unified_loader,group,sets,binary,global_group,file_server,proplists,
 file_io_server,beam_lib,heart,erl_distribution,disk_server,beam_utils,
 '9p_server','9p_zero',ling_disasm,ling_iops,ling_iopvars,ling_lib,
 application_controller,application_master,net_kernel,prim_inet,ling_bifs,
 ling_code,global,prim_file,net_vif,ets,linc_max,linc_max_generator,**flow0**,
 linc_max_headers,error_handler,error_logger,erl_prim_loader,erlang,erl_scan,
 erl_syntax,erl_parse,'9p_mounter',erl_eval,console,v3_kernel,v3_life,sys,
 supervisor,supervisor_bridge,sys_pre_expand,v3_core,v3_codegen,shell,compile,
 sys_core_fold,sys_core_dsetel,shell_default,proc_lib,code_server,erl_bifs,
 beam_opcodes,application,'9p',code,orddict,otp_internal,core_lib,string,
 io_lib_format,io_lib_pretty,beam_type,beam_z,beam_validator,lib,gen,
 eval_bits,beam_split,beam_peep,beam_receive,beam_trim,gen_server,
 error_logger_tty_h,beam_jump,beam_except,beam_flatten,gen_event,os,gb_trees,
 gb_sets,erl_lint,erl_internal,cerl,epp,ordsets,erl_bits,erl_expand_records,
 beam_clean,beam_dead,beam_dict,edlin,beam_block,beam_bool,beam_bsm,beam_asm,
 io_lib,io,beam_a,c,unicode,user_sup,sofs,init,rpc,standard_error,user_drv,
 kernel,inet_udp,inet,kernel_config]ok
```

The dynamically-generated flow0 module shows on the list of loaded modules. It
does not show in the output of the m() command because its loading did not use
the code server.

The processing delay stayed essentially the same:
PL (all) = 5.28 +/- 0.62 us (95%)
PL (to) = 5.78 +/- 1.01 us (95%)
PL (from) = 4.78 +/- 0.60 us (95%)

----[09/02/2014 11:52]----------------------------------------------------------

The generator is partially rewritten to decrease compilation time. Now it
produces a single function with 24 arguments, not 7 functions with 13-17
arguments.

There is no noticable impact on the processing delay:

PL (all) = 5.07 +/- 1.08 us (95%)
PL (to) = 5.20 +/- 1.72 us (95%)
PL (from) = 4.93 +/- 1.32 us (95%)

The compilation of a fairly complex flow table (test3.tab) now takes 344ms.

----[14/02/2014 00:48]----------------------------------------------------------

Let us add unit tests to the packet preparser.

The spec demands that match on IN_PHY_PORT is only possible if IN_PORT is
present. Let other module suppress the physical port info information before
calling linc_max:inject(). This is *not checked* by the preparser.

The preparser and generator test suites complete:

1> eunit:test(linc_max_preparser).
  All 47 tests passed.
ok
2> eunit:test(linc_max_generator).
  All 302 tests passed.
ok

----[20/02/2014 00:12]----------------------------------------------------------

The initial integration of the new fast path into the LINC switch is complete.

The following pieces of functionality are missing/not-working:

* queues
* meters
* tx/rx counters
* packet modification (Set-Field, etc)

Other issues:

* linc_max uses a single set of flow tables for all switches (flow_table_0,
flow_table_1, ...).

The Set-Field action asks for a faster implementation. The best approach is to
use a scanner similar to linc_max_preparser to identify the range withing the
packet that corresponding the the field being modified. The a new binary can be
constructed by concatenating the beginning of the original packet, the new field
value and the rest of the original.

This rises an issue with checksum calculation. Checksums is a pain point for the
LINC switch. Most probably, it can be optimized too to get the implementation of
the Set-Field action that runs the fastest. This is a bit of work. For the time
being, we will use the pkt module to perform Set-Field actions.

----[21/02/2014]----------------------------------------------------------------

The first iteration of packet-modifying actions is complete and covered with
unit tests. The implementation uses pkt:decapsulate/encapsulate and thus is
slow. How slow?

A test pushes a VLAN tag and pops it immediately before forwarding the packet.
The processing delay grows:

5> ling:experimental(processing_delay, []).
Processing delay statistics:
Packets: 2000
Delay: 13.842us +- 0.443 (95%)
ok

And iperf shows TCP throughput of ~200Mbit/s.
The memory consumption stays low after the iperf run:

9> memory().
[{total,19007780},
 {processes,2543616},
 {system,16464164},
 {processes_used,757812},
 {atom,1033416},
 {atom_used,839928},
 {binary,10735616},
 {code,4564060},
 {ets,131072}]

It looks like Linux networking code adapts to the speed of an interfaces. Higher
processing delay decreases the throughput abruptly.

Now that we have the test suite for the linc_max_fast_actions module and the
performance figures, we are ready to make the packet-modification fast (by
abandoning pkt:*).

----[24.02.2014 03:55]----------------------------------------------------------

The fast packet modification code complete and covered with unit tests. Let us
measure the latency cost of linc_max_splicer.

We measure the processing delay using ping packets. These packets have Ethernet,
IPv4, and ICMPv4 headers. The modifiable fields are thus:

* eth\_src
* eth\_dst
* ipv4\_src
* ipv4\_dst
* ip\_dscp
* ip\_ecn

Other fields, e.g. icmpv4_type cannot be modified without destroying the packet.
linc_max_splicer returns 'protected' for such fields.

The testing involves manual modification of priv/test5.tab flow table. A
set-field action added 10 times to both forward and opposite flow. The latency
is measured using `ling:experimental(processing_delay, [])`.

All times in the table are in microseconds.

Field | N | Base | Delay
------|---|------|------
eth\_dst| 10 | 1.2 | 5.3
eth\_src| 10 | 1.2 | 4.7
ipv4\_src | 10 | 1.7 | 17.6
ipv4\_dst | 10 | 1.2 | 17.4
ip\_dscp | 10 | 1.2 | 17.7
ip\_ecn | 10 | 1.4 | 17.0

